# This Agent is responsible for generating responses using LLM based on user queries and retrieved document chunks.

import os
import sys
from dotenv import load_dotenv
from src.logger import logging
from src.exception import CustomException
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from typing import List
from langchain_core.documents import Document

# Load environment variables from the .env file (API key, model name, etc.)
load_dotenv()

class LLMResponseAgent:
    def __init__(self):
        """
        Initializes the LLMResponseAgent with a language model (via OpenRouter - Mistral).
        It uses credentials from the .env file to authenticate the LLM API.
        """
        try:
            # Load model name and API key from environment
            self.model_name = os.getenv("MISTRAL_MODEL_NAME")
            self.api_key = os.getenv("OPENROUTER_API_KEY")

            if not self.api_key:
                raise ValueError(
                    "OPENROUTER_API_KEY not found. "
                    "Please make sure it is set in your .env file."
                )

            # Initialize the LLM (ChatOpenAI compatible with OpenRouter)
            self.llm = ChatOpenAI(
                model=self.model_name,
                openai_api_key=self.api_key,
                openai_api_base="https://openrouter.ai/api/v1",  # Base URL for OpenRouter
                temperature=0.3,     # Lower temperature for more deterministic output
                max_tokens=2000      # Limit token size to prevent overflow
            )

            logging.info(f"LLMResponseAgent initialized with model: {self.model_name}")
        
        except Exception as e:
            raise CustomException(e, sys)

    def generate_response(self, query: str, retrieved_docs: List[Document], trace_id: str) -> dict:
        """
        Generates a structured response by prompting the LLM with relevant context and the user query.

        Args:
            query (str): The user's input question.
            retrieved_docs (List[Document]): List of retrieved context chunks (from vector DB).
            trace_id (str): Unique identifier for tracking the flow across agents.

        Output:
            dict: A structured message containing the generated answer and query.
        """
        try:
            # --- Build prompt context from retrieved documents ---
            context_parts = []
            for doc in retrieved_docs:
                source = doc.metadata.get('source', 'Unknown Document')
                # Add source label to each context chunk
                context_parts.append(f"--- Context from: {source} ---\n{doc.page_content}")
            
            # Join all chunks into a single context block
            context = "\n\n".join(context_parts)
            
            if not context:
                # If no context is available, return a fallback answer
                logging.warning("No context provided to LLM, generating response based on query alone.")
                answer = "I could not find any relevant information in the uploaded documents to answer your question."
            else:
                # Call the method to generate a detailed LLM answer
                answer = self.generate_answer(context, query)
            
            # Return a structured message for downstream use (e.g., by CoordinatorAgent)
            return {
                "sender": "LLMResponseAgent",
                "receiver": "CoordinatorAgent",
                "type": "LLM_RESPONSE",
                "trace_id": trace_id,
                "payload": {
                    "answer": answer,
                    "query": query
                }
            }
        except Exception as e:
            raise CustomException(e, sys)

    def generate_answer(self, context: str, query: str) -> str:
        """
        Constructs a detailed prompt combining context and question,
        then invokes the LLM to generate a precise and source-referenced answer.

        Args:
            context (str): Text extracted from the documents.
            query (str): The user's input question.

        Output:
            str: The final response generated by the LLM.
        """
        try:
            # Build a highly instructive prompt for the LLM
            prompt = f"""You are a helpful and precise assistant. Use the following context, which is composed of sections from different documents, to answer the question. 
Your answer should be comprehensive and synthesize information from all relevant sources provided. 
Explicitly mention the source document (e.g., 'According to Jinil_Patel_Resume.pdf...') when the information is specific to one file.

CONTEXT:
{context}

QUESTION: {query}

INSTRUCTIONS:
1. Base your answer *only* on the provided context.
2. If the context contains information from multiple documents, synthesize it into a single, coherent answer.
3. Be specific and mention concrete details, skills, or concepts from the context.
4. If the context does not contain enough information to answer the question, clearly state that the information is not available in the provided documents.
5. Do not make up information.

ANSWER:"""

            # Wrap the prompt as a message for the LLM
            messages = [HumanMessage(content=prompt)]

            # Generate the response from the LLM
            response = self.llm.invoke(messages)

            return response.content.strip()

        except Exception as e:
            logging.error(f"Error generating answer: {str(e)}")
            raise CustomException(e, sys)
